{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pathlib import Path\n",
    "import plotly.express as px\n",
    "#import plotly\n",
    "#from os import path\n",
    "import missingno\n",
    "from statsmodels.formula.api import ols\n",
    "#import datetime\n",
    "\n",
    "# import project modules\n",
    "import download_data\n",
    "#import stats_functions\n",
    "#import screening\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load carbon intensity data and visualize missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the regions we want to examine\n",
    "\n",
    "ba_list = ['EIA.CISO', 'EIA.ISNE', 'EIA.BPAT','EIA.MISO', 'EIA.NYIS', 'EIA.PJM', 'EIA.SWPP',\n",
    "'AEC', 'AECI', 'AVA', 'AZPS', 'BANC', 'CHPD', 'CPLE', 'CPLW', 'DOPD', 'DUK', 'EPE', 'ERCO', 'FMPP', \n",
    "'FPC', 'FPL', 'GCPD', 'GVL', 'HST','IID', 'IPCO', 'JEA', 'LDWP', 'LGEE', 'NEVP', 'NSB',\n",
    "'NWMT', 'PACE', 'PACW', 'PGE', 'PNM', 'PSCO', 'PSEI', 'SC', 'SCEG', 'SCL', 'SEC', \n",
    "'SOCO', 'SPA', 'SRP', 'TAL', 'TEC', 'TEPC', 'TIDC', 'TPWR', 'TVA', 'WACM', 'WALC', 'WAUW']\n",
    "\n",
    "\n",
    "ef_year = 2019\n",
    "\n",
    "# specify the type of emission factor we want to use\n",
    "ef_type = 'consumption_ef_EGRID_2019'\n",
    "\n",
    "# choose the units you want to use -either kgCO2/kWh or lbCO2/MWh\n",
    "units = 'lbCO2/MWh'\n",
    "\n",
    "# the base units are kg/kWh\n",
    "unit_conversion = 1\n",
    "if units == 'lbCO2/MWh':\n",
    "    unit_conversion = 1 / 0.453592 * 1000\n",
    "\n",
    "print(f'Year = {ef_year}')\n",
    "print(f'Number of BAs to load: {len(ba_list)}')\n",
    "\n",
    "# Load the hourly emission factors\n",
    "##################################\n",
    "hourly_ef = load_data.load_hourly_efs(ba_list, ef_year, ef_type)\n",
    "\n",
    "# rename columns to remove EIA. prefix\n",
    "hourly_ef.columns = [col.split('.')[-1] for col in hourly_ef.columns]\n",
    "# update list of regions\n",
    "ba_list = list(hourly_ef.columns)\n",
    "\n",
    "# remove negative values\n",
    "hourly_ef[hourly_ef < 0] = np.NaN\n",
    "\n",
    "# create a dictionary that matches BA codes to names\n",
    "ba_name_dict = pd.read_csv('../data/manual/ba_names.csv', index_col='ba_code').to_dict()['ba_name']\n",
    "\n",
    "print(f'Number of BAs available: {len(ba_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(hourly_ef, labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop BAs with all missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BAs with all data missing were dropped:')\n",
    "print(hourly_ef.isna().sum()[hourly_ef.isna().sum() == 8760])\n",
    "\n",
    "# drop any columns with all missing values\n",
    "hourly_ef = hourly_ef.dropna(axis=1, how='all')\n",
    "\n",
    "#update the BA list\n",
    "ba_list = list(hourly_ef.columns)\n",
    "\n",
    "print(f'Number of BAs available: {len(ba_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Missing Values from EIA-930\n",
    "Carbonara had filled missing data by filling forward gaps of less than 24 hours. Using the EIA-930 data, we want to identify where the missing source data exists, and use this to restore the missing values in the EF dataset so that we can perform our own interpolation of the missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variance statistics for each BA for the Carbonara data(this will be used later)\n",
    "variance_carbonara = pd.DataFrame()\n",
    "variance_carbonara['stdev'] = hourly_ef.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from EIA-930 for 2019\n",
    "columns_to_use = ['Balancing Authority', 'UTC Time at End of Hour',\n",
    "       'Net Generation (MW) from Coal', 'Net Generation (MW) from Natural Gas',\n",
    "       'Net Generation (MW) from Nuclear',\n",
    "       'Net Generation (MW) from All Petroleum Products',\n",
    "       'Net Generation (MW) from Hydropower and Pumped Storage',\n",
    "       'Net Generation (MW) from Solar', 'Net Generation (MW) from Wind',\n",
    "       'Net Generation (MW) from Other Fuel Sources',\n",
    "       'Net Generation (MW) from Unknown Fuel Sources']\n",
    "\n",
    "eia_930 = pd.concat([pd.read_csv('../data/downloaded/eia/EIA930_BALANCE_2019_Jan_Jun.csv', usecols=columns_to_use, thousands=',', parse_dates=['UTC Time at End of Hour']),\n",
    "                     pd.read_csv('../data/downloaded/eia/EIA930_BALANCE_2019_Jul_Dec.csv', usecols=columns_to_use, thousands=',', parse_dates=['UTC Time at End of Hour'])])\n",
    "\n",
    "#only keep regions that are in the BA list\n",
    "eia_930 = eia_930[eia_930['Balancing Authority'].isin(ba_list)]\n",
    "\n",
    "# get a list of unique BAs in this dataset\n",
    "bas_in_930 = list(eia_930['Balancing Authority'].unique())\n",
    "\n",
    "# convert from end of hour timestamp to beginning of hour timestamp\n",
    "eia_930['UTC Time at End of Hour'] = eia_930['UTC Time at End of Hour'] - pd.Timedelta(hours=1)\n",
    "\n",
    "# localize the timezone as UTC time\n",
    "eia_930['UTC Time at End of Hour'] = eia_930['UTC Time at End of Hour'].dt.tz_localize('UTC')\n",
    "\n",
    "# we want to create a datetime column that is in local standard time\n",
    "\n",
    "# create a column for local time\n",
    "eia_930['Local Time'] = eia_930['UTC Time at End of Hour']\n",
    "\n",
    "# convert from UTC time to local time\n",
    "for ba in bas_in_930:\n",
    "    # get the time zone\n",
    "    local_tz = download_data.ba_timezone(ba, 'GMT')\n",
    "\n",
    "    # populate the local time column, then strip the TZ info from the data\n",
    "    eia_930.loc[eia_930['Balancing Authority'] == ba, 'Local Time'] = eia_930.loc[eia_930['Balancing Authority'] == ba, 'UTC Time at End of Hour'].dt.tz_convert(local_tz).dt.tz_localize(None)\n",
    "\n",
    "# drop the UTC column, and set the BA and local time as index\n",
    "eia_930 = eia_930.drop(columns=['UTC Time at End of Hour'])\n",
    "eia_930 = eia_930.set_index(['Balancing Authority','Local Time'])\n",
    "\n",
    "eia_930.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where there is no net generation data\n",
    "missing_930 = eia_930[eia_930.sum(axis=1) == 0].reset_index()\n",
    "\n",
    "# get a list of all BAs that have missing net generation data\n",
    "missing_bas = list(missing_930['Balancing Authority'].unique())\n",
    "\n",
    "# for each BA, replace the filled values in hourly_ef with missing values where the source data is missing\n",
    "for ba in missing_bas:\n",
    "    # get the datetimes that are missing and replace with NaN\n",
    "    hourly_ef.loc[missing_930[missing_930['Balancing Authority'] == ba]['Local Time'], ba] = np.NaN\n",
    "\n",
    "# remove negative values\n",
    "hourly_ef[hourly_ef < 0] = np.NaN\n",
    "\n",
    "print('BAs with all data missing were dropped:')\n",
    "print(hourly_ef.isna().sum()[hourly_ef.isna().sum() == 8760])\n",
    "\n",
    "# drop any columns with all missing values\n",
    "hourly_ef = hourly_ef.dropna(axis=1, how='all')\n",
    "\n",
    "#update the BA list\n",
    "ba_list = list(hourly_ef.columns)\n",
    "\n",
    "print(f'Number of BAs available: {len(ba_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(hourly_ef, labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the performance of different interpolation methods\n",
    "When filling missing data, we want to be careful to not affect the variance of the data too much, since this could bias our hourly inventory results.\n",
    "We will check several filling methods to see how much this impacts the variance compared to the original data with all the missing values intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate variance stats for the restored missing data\n",
    "variance_missing = pd.DataFrame()\n",
    "variance_missing['stdev'] = hourly_ef.std(ddof=0)\n",
    "\n",
    "# calculate the mean absolute percent difference between the raw data and carbonara data\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_carbonara - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_carbonara - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill-forward\n",
    "Filling forward all data makes the percent difference worse at 0.34%, which makes sense since we are filling more missing values than Carbonara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ef_ffill = hourly_ef.copy().fillna(method='ffill')\n",
    "\n",
    "variance_ffill = pd.DataFrame()\n",
    "variance_ffill['stdev'] = ef_ffill.std(ddof=0)\n",
    "\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_ffill - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_ffill - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Interpolation\n",
    "Linear interpolation is a slight improvement to 0.29%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_linear = hourly_ef.copy().interpolate(method='linear')\n",
    "\n",
    "variance_linear = pd.DataFrame()\n",
    "variance_linear['stdev'] = ef_linear.std(ddof=0)\n",
    "\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_linear - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_linear - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average\n",
    "Using a 4-day, centered moving average, which itself is linearly interpolated for larger gaps reduces the percent difference to 0.20%.\n",
    "\n",
    "We tested several windows (24, 36, 48, 72, 96, 120, and 168 hours) and this seemed to perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_rolling = hourly_ef.copy().fillna(hourly_ef.rolling(window=96, center=True, min_periods=24, axis=0).mean().interpolate(method='linear'))\n",
    "\n",
    "variance_rolling = pd.DataFrame()\n",
    "variance_rolling['stdev'] = ef_rolling.std(ddof=0)\n",
    "\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_rolling - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_rolling - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average plus hourly fixed effect\n",
    "Could adding an hourly fixed effect to the moving average improve this further?\n",
    "For each BA that has missing values, calculate the moving average, and also construct an hourly fixed effect model for the whole year. Then add the hourly fixed effect to the moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_hourly_MA = hourly_ef.copy()\n",
    "\n",
    "# for each BA that has any missing values\n",
    "for ba in list(ef_hourly_MA.columns):\n",
    "    if ef_hourly_MA[ba].isna().any():\n",
    "        # create a df with only that BA\n",
    "        hourly_model = ef_hourly_MA.copy()[[ba]]\n",
    "        # ad a column for the hour\n",
    "        hourly_model['hour'] = hourly_model.index.hour\n",
    "        # calculate a model\n",
    "        model = ols(f'{ba} ~ C(hour)', data=hourly_model).fit()\n",
    "        # get a list of the coefficients\n",
    "        params = model.params.values\n",
    "        # replace the first value (the intercept) with 0\n",
    "        params[0] = 0.0\n",
    "        # add these values to the df\n",
    "        hourly_model['fixed_effect'] = np.tile(params, 365)\n",
    "        # calculate the moving average, and linearly interpolate any gaps in this data\n",
    "        hourly_model['MA'] = ef_hourly_MA[ba].rolling(window=96, center=True, min_periods=24, axis=0).mean().interpolate(method='linear')\n",
    "        hourly_model['filled'] = hourly_model[ba].fillna(hourly_model['MA'] + hourly_model['fixed_effect'])\n",
    "\n",
    "        # fill the data with the missing values\n",
    "        ef_hourly_MA[ba] = ef_hourly_MA[ba].fillna(hourly_model['filled'])\n",
    "\n",
    "variance_hourly_MA = pd.DataFrame()\n",
    "variance_hourly_MA['stdev'] = ef_hourly_MA.std(ddof=0)\n",
    "\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_hourly_MA - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_hourly_MA - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monthly TOD average\n",
    "Fill with the monthly time of day average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_mh = hourly_ef.copy()\n",
    "\n",
    "# calculate the month-hour average\n",
    "mh_average = ef_mh.copy()\n",
    "mh_average['month'] = mh_average.index.month\n",
    "mh_average['hour'] = mh_average.index.hour\n",
    "mh_average = mh_average.groupby(['month','hour']).mean().reset_index()\n",
    "\n",
    "mh_hold = pd.DataFrame(index=ef_mh.index)\n",
    "mh_hold['month'] = mh_hold.index.month\n",
    "mh_hold['hour'] = mh_hold.index.hour\n",
    "\n",
    "# merge month-hourly\n",
    "mh_hold = mh_hold.merge(mh_average, how='left', on=['month','hour']).set_index(ef_mh.index).drop(columns=['month','hour'])\n",
    "\n",
    "# fill with the month hourly average\n",
    "for ba in list(ef_mh.columns):\n",
    "    ef_mh[ba] = ef_mh[ba].fillna(mh_hold[ba])\n",
    "\n",
    "variance_mh = pd.DataFrame()\n",
    "variance_mh['stdev'] = ef_mh.std(ddof=0)\n",
    "\n",
    "print(f'Mean Absolute percentage difference: {round(abs(((variance_mh - variance_missing) / variance_missing)).mean().values[0] *100, 2)}%')\n",
    "print(f'Mean percentage difference: {round(((variance_mh - variance_missing) / variance_missing).mean().values[0] *100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the interpolation for a single region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba= 'GVL'\n",
    "test = hourly_ef.copy()[[ba]]\n",
    "test['ffill'] = test[ba].fillna(ef_ffill[ba])\n",
    "test['linear'] = test[ba].fillna(ef_linear[ba])\n",
    "test['MA'] = test[ba].fillna(ef_rolling[ba])\n",
    "test['MA_fixed_effect'] = test[ba].fillna(ef_hourly_MA[ba])\n",
    "test['monthlyTOD'] = test[ba].fillna(ef_mh[ba])\n",
    "\n",
    "\n",
    "px.line(test, y=['ffill','linear','MA','MA_fixed_effect','monthlyTOD',ba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the interpolated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_ef.to_csv('../data/processed/emission_factors/emission_factors_raw.csv')\n",
    "ef_ffill.to_csv('../data/processed/emission_factors/emission_factors_ffill.csv')\n",
    "ef_linear.to_csv('../data/processed/emission_factors/emission_factors_linear.csv')\n",
    "ef_rolling.to_csv('../data/processed/emission_factors/emission_factors_MA.csv')\n",
    "ef_hourly_MA.to_csv('../data/processed/emission_factors/emission_factors_fixed_effects.csv')\n",
    "ef_mh.to_csv('../data/processed/emission_factors/emission_factors_monthhour.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbfb11d76ba7fcd62de728492717c2999a3c11ae1c83ea329d6e038c7a5b35c0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('emissions': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
