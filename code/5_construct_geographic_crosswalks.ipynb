{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "from shapely.ops import nearest_points\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In order to identify which buildings in the NREL EULP dataset are located in each climate zone in each balancing authority (to select our stratified random sample), we need to identify all of the census tract IDs and TMY3 locations that are located in each BA-climate zone pair.\n",
    "\n",
    "The NREL commercial building metadata identifies an NHGIS census tract identifier for each commercial building, and the residential building metadata identifies a \"weather file location\" (which is a TMY3 location) for each residential building.\n",
    "\n",
    "Before creating this, we check that Carbonara CI data is available for each BA in which we are interested. If not, we remove that BA. \n",
    "\n",
    "### Downloaded data sources\n",
    "United States Environmental Protection Agency (EPA). 2022. “Emissions & Generation Resource Integrated Database (eGRID), 2020” Washington, DC: Office of Atmospheric Programs, Clean Air Markets Division. Available from EPA’s eGRID web site: https://www.epa.gov/egrid.\n",
    "- eGRID2019 data: https://www.epa.gov/sites/default/files/2021-02/egrid2019_data.xlsx\n",
    "\n",
    "United States Energy Information Administration (EIA). \"Hourly Electric Grid Monitor (EIA Form-930).\" Available from: https://www.eia.gov/electricity/gridmonitor/about\n",
    "- 2019 generation balance data from EIA-930: https://www.eia.gov/electricity/gridmonitor/sixMonthFiles/EIA930_BALANCE_2019_Jul_Dec.csv and https://www.eia.gov/electricity/gridmonitor/sixMonthFiles/EIA930_BALANCE_2019_Jan_Jun.csv\n",
    "\n",
    "\n",
    "GIS shapefiles downloaded from the U.S. Department of Homeland Security's \"Homeland Infrastructure Foundation-Level Data (HIFLD)\"\n",
    "\n",
    "- Electric Planning Areas: https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::electric-planning-areas-1/explore (NOTE: In this dataset, the PACW and PACE BAs are combined as a single multipolygon, as are the CPLW and CPLE BAs. Prior to loading this shapefile into this notebook, the authors manually separated these multiploygons into their component parts using QGIS software)\n",
    "\n",
    "GIS Shapefiles downloaded from the U.S. Energy Information Administration's \"U.S. Energy Atlas\"\n",
    "- Climate Zones - DOE Building America Program: https://atlas.eia.gov/datasets/eia::climate-zones-doe-building-america-program/explore?location=35.902577%2C-95.221420%2C4.90 (NOTE: this shapefile was missing climate zone data for the southern tip of Florida (indluding Miami), so the author manually corrected this in QGIS)\n",
    "\n",
    "GIS Shapefiles from  Steven Manson, Jonathan Schroeder, David Van Riper, Tracy Kugler, and Steven Ruggles. IPUMS National Historical Geographic Information System: Version 16.0 [dataset]. Minneapolis, MN: IPUMS. 2021. http://doi.org/10.18128/D050.V16.0\n",
    "- 2019 U.S. Census Tracts\n",
    "\n",
    "Wilson et al. 2021. End-Use Load Profiles for the U.S. Building Stock: Methodology and Results of Model Calibration, Validation, and Uncertainty Quantification. NREL/TP-5500-80889 (forthcoming report). \n",
    "- Commercial and Residential metadata files\n",
    "\n",
    "### Data inputs that were manually created\n",
    "- `ba_tz.csv` identifies the UTC timezone offset for each balancing area in the US\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Determine which Balancing Areas to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BA metadata from eGRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get some metadata about the BAs from eGRID\n",
    "column_names = {'BANAME':'ba_name',\n",
    "                'BACODE':'ba_code',\n",
    "                'BANGENAN':'net_generation',\n",
    "                'BACO2AN':'emissions'}\n",
    "ba_meta = pd.read_excel('../data/downloaded/egrid/egrid2019_data.xlsx', sheet_name='BA19', header=1, usecols=['BANAME','BACODE','BANGENAN','BACO2AN']).rename(columns=column_names)\n",
    "ba_meta.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load information about the timezone offset for each BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tz offset for each BA for use when downloading singularity data\n",
    "if path.exists(f'../data/manual/ba_tz.csv'):\n",
    "    pass\n",
    "else:\n",
    "    tz_offsets = pd.read_csv('../data/downloaded/eia/EIA930_BALANCE_2019_Jan_Jun.csv', usecols=['Balancing Authority','Local Time at End of Hour','UTC Time at End of Hour'], parse_dates=['Local Time at End of Hour','UTC Time at End of Hour'])\n",
    "\n",
    "    tz_offsets = tz_offsets.drop_duplicates(subset='Balancing Authority', keep='first')\n",
    "\n",
    "    tz_offsets['offset'] =  (tz_offsets['UTC Time at End of Hour'] - tz_offsets['Local Time at End of Hour']).astype('timedelta64[h]').astype(int)\n",
    "\n",
    "    print(dict(zip(tz_offsets['Balancing Authority'], tz_offsets['offset'])))\n",
    "\n",
    "    tz_offsets.to_csv('../data/manual/ba_tz.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EIA-930 data for 2019 and get sums for each BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = ['Balancing Authority', 'Data Date', 'Hour Number',\n",
    "     'Demand (MW) (Adjusted)', 'Net Generation (MW) (Adjusted)', 'Net Generation (MW)',\n",
    "       'Net Generation (MW) from Coal', 'Net Generation (MW) from Natural Gas',\n",
    "       'Net Generation (MW) from Nuclear',\n",
    "       'Net Generation (MW) from All Petroleum Products',\n",
    "       'Net Generation (MW) from Hydropower and Pumped Storage',\n",
    "       'Net Generation (MW) from Solar', 'Net Generation (MW) from Wind',\n",
    "       'Net Generation (MW) from Other Fuel Sources',\n",
    "       'Net Generation (MW) from Unknown Fuel Sources']\n",
    "\n",
    "# load the data from EIA-930 for 2019\n",
    "eia_930 = pd.concat([pd.read_csv('../data/downloaded/eia/EIA930_BALANCE_2019_Jan_Jun.csv', usecols=columns_to_use, thousands=','),pd.read_csv('../data/downloaded/eia/EIA930_BALANCE_2019_Jul_Dec.csv', usecols=columns_to_use, thousands=',')])\n",
    "\n",
    "# sum by balancing authority\n",
    "eia_930 = eia_930.groupby('Balancing Authority').sum()\n",
    "eia_930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify relevant BAs and view size of BA by total demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any generation-only balancing authorities (according to https://www.eia.gov/electricity/gridmonitor/about)\n",
    "gen_only_bas = ['AVRN','DEAA','EEI','GRID','GRIF','GWA','HGMA','SEPA','WWA','YAD'] #NOTE: GRMA also generation-only, but retired in 2018\n",
    "eia_930 = eia_930.drop(gen_only_bas)\n",
    "\n",
    "# add the remaining BAs to the list that we should consider\n",
    "bas_to_consider = list(eia_930.index)\n",
    "\n",
    "eia_930['Percent Demand'] = eia_930['Demand (MW) (Adjusted)'] / eia_930['Demand (MW) (Adjusted)'].sum() * 100\n",
    "\n",
    "eia_930.sort_values(by='Demand (MW) (Adjusted)', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Carbonara data is available for all of our relevant BAs\n",
    "NOTE: This only needs to be run once. We ran it and identified that all BAs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine for which of these BAs data is available in the singularity API\n",
    "pass\n",
    "\n",
    "\"\"\"# LOAD API CREDENTIALS from CSV file\n",
    "api_credentials = pd.read_csv('../data/api_credentials.csv', index_col='API').to_dict()\n",
    "api_key = api_credentials['PASSWORD']['Singularity']\n",
    "\n",
    "# define parameters for API call\n",
    "event_type = 'carbon_intensity'\n",
    "header = {'X-Api-Key': api_key}\n",
    "\n",
    "start='2019-02-02T00:00:00%2B00:00'\n",
    "end='2019-02-02T16:45:00%2B00:00'\n",
    "\n",
    "data_exists = []\n",
    "data_dne = []\n",
    "for ba in bas_to_consider:\n",
    "\n",
    "    output = requests.get(f'https://api.singularity.energy/v1/region_events/search?region={ba}&start={start}&end={end}&event_type={event_type}&per_page=1000&page=1', headers=header)\n",
    "    df = pd.json_normalize(output.json(), 'data')\n",
    "    if df.empty: \n",
    "        eia_ba = f'EIA.{ba}'\n",
    "        # see if the EIA-appended version exists\n",
    "        output = requests.get(f'https://api.singularity.energy/v1/region_events/search?region={eia_ba}&start={start}&end={end}&event_type={event_type}&per_page=1000&page=1', headers=header)\n",
    "        df = pd.json_normalize(output.json(), 'data')\n",
    "        if df.empty: \n",
    "            data_dne.append(ba)\n",
    "            print(f'{ba}: No Data')\n",
    "        else: \n",
    "            data_exists.append(ba)\n",
    "            print(f'{ba}: Data Exists ({eia_ba})')\n",
    "    else: \n",
    "        data_exists.append(ba)\n",
    "        print(f'{ba}: Data Exists')\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Identify the set of all climate zones in each BA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the BAs that are not included in the GIS shapefile and will have to be manually inputted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all of the BAs from EIA-930 are included in our shapefile\n",
    "\n",
    "# load the balancing area data\n",
    "ba_shp = gpd.read_file('../data/downloaded/gis_shapefiles/Electric_Planning_Areas/Planning_Areas.shp')[['NAME','ABBRV','geometry']]\n",
    "# rename some columns\n",
    "ba_shp = ba_shp.rename(columns={'NAME':'ba_name','ABBRV':'ba_code'})\n",
    "\n",
    "# get a list of all BAs in our shapefile\n",
    "bas_in_shp = list(ba_shp.ba_code.unique())\n",
    "\n",
    "# list all of the BAs of interest that are not in the shapefile\n",
    "missing_bas = [i for i in bas_to_consider if i not in bas_in_shp]\n",
    "\n",
    "ba_meta[ba_meta['ba_code'].isin(missing_bas)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GeoDataFrame of unique BA-Climate Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all climate zones in each BA\n",
    "# load the balancing area data\n",
    "ba_shp = gpd.read_file('../data/downloaded/gis_shapefiles/Electric_Planning_Areas/Planning_Areas.shp')[['NAME','ABBRV','geometry']]\n",
    "# rename some columns\n",
    "ba_shp = ba_shp.rename(columns={'NAME':'ba_name','ABBRV':'ba_code'})\n",
    "\n",
    "# only keep the BAs in our list\n",
    "ba_shp = ba_shp[ba_shp['ba_code'].isin(bas_to_consider)]\n",
    "\n",
    "# import climate zone data and create a new zone column that combines the climate and moisture codes\n",
    "climate_zones = gpd.read_file('../data/manual/gis/climate_zones_edited.shp').replace('N/A', '').replace('None', '')\n",
    "climate_zones['climate_zone'] = climate_zones['IECC_Clima'].astype(str) + climate_zones['IECC_Moist'].astype(str).replace('None', '')\n",
    "\n",
    "# create a new gdf that intersects balancing areas and climate zones\n",
    "# this will give us a unique polygon for each climate zone in each BA\n",
    "ba_cz = gpd.overlay(ba_shp.to_crs('EPSG:4326'), climate_zones, how='intersection')\n",
    "\n",
    "# only keep certain columns\n",
    "ba_cz = ba_cz[['ba_name','ba_code','climate_zone','geometry']]\n",
    "\n",
    "# calculate the area of each bz_cz\n",
    "ba_cz['area'] = ba_cz.area\n",
    "\n",
    "# set the index to be ba_code_cz\n",
    "ba_cz = ba_cz.set_index(['ba_code','climate_zone'])\n",
    "\n",
    "# calculate the percent of area that each cz is\n",
    "ba_cz['cz_pct_of_ba_area'] = ba_cz['area'] / ba_cz.groupby(['ba_code']).sum()['area']\n",
    "\n",
    "# drop any climate zones that are less than 1% of the BA area\n",
    "ba_cz = ba_cz[ba_cz['cz_pct_of_ba_area'] >= 0.01]\n",
    "\n",
    "# drop the area and pct columns\n",
    "ba_cz = ba_cz.drop(columns=['area','cz_pct_of_ba_area'])\n",
    "\n",
    "ba_cz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Identify all of the census tracts in each BA-CZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the shapefile containing all of the census tracts\n",
    "tracts = gpd.read_file('../data/downloaded/gis_shapefiles/nhgis0001_shapefile_tl2019_us_tract_2019/US_tract_2019.shp')[['GISJOIN','geometry']]\n",
    "\n",
    "# change the crs to match\n",
    "tracts = tracts.to_crs('EPSG:4326')\n",
    "\n",
    "tracts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which tracts are located in each BA_CZ\n",
    "ba_cz_tract = gpd.sjoin(ba_cz, tracts, how='left', op='intersects')\n",
    "\n",
    "# drop columns and reset index\n",
    "ba_cz_tract = ba_cz_tract.drop(columns=['geometry','index_right'])\n",
    "ba_cz_tract = ba_cz_tract.reset_index()\n",
    "\n",
    "ba_cz_tract.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_cz_tract.to_csv('../data/processed/ba_tract_crosswalk_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Identify all of the weather file locations in each BA-CZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_meta_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/metadata/metadata.parquet'\n",
    "weather_file_locations = pd.read_parquet(res_meta_url, columns=['in.weather_file_city','in.weather_file_latitude','in.weather_file_longitude'])\n",
    "weather_file_locations = weather_file_locations.drop_duplicates()\n",
    "# convert to a geodataframe\n",
    "weather_file_locations = gpd.GeoDataFrame(weather_file_locations, geometry=gpd.points_from_xy(weather_file_locations['in.weather_file_longitude'], weather_file_locations['in.weather_file_latitude']))\n",
    "weather_file_locations = weather_file_locations.set_crs('EPSG:4326')\n",
    "\n",
    "# add climate zone to each location\n",
    "#weather_file_locations = gpd.sjoin(weather_file_locations, climate_zones[['geometry','climate_zone']], how='left', op='intersects')\n",
    "#weather_file_locations = weather_file_locations.drop(columns=['index_right'])\n",
    "weather_file_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which weather files are located in each BA_CZ\n",
    "ba_cz_wf_location = gpd.sjoin(ba_cz, weather_file_locations, how='left', op='contains')\n",
    "ba_cz_wf_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_file_points = weather_file_locations.set_index('in.weather_file_city')\n",
    "\n",
    "# get a list of all indexes that are missing\n",
    "missing_wf_list = list(ba_cz_wf_location[ba_cz_wf_location['in.weather_file_city'].isna()].index)\n",
    "\n",
    "for missing in missing_wf_list:\n",
    "    # get the missing polygon\n",
    "    missing_polygon = missing_wf.loc[missing,'geometry']\n",
    "    # find the distance to each point\n",
    "    closest_points = weather_file_locations.set_index('in.weather_file_city').distance(missing_polygon).sort_values(ascending=True)\n",
    "    # find the closest location to the polygon\n",
    "    closest = closest_points.index[0]\n",
    "    # add the closest location to the dataframe\n",
    "    ba_cz_wf_location.loc[missing,'in.weather_file_city'] = closest\n",
    "\n",
    "ba_cz_wf_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any duplicate rows\n",
    "ba_cz_wf_location = ba_cz_wf_location.reset_index()\n",
    "ba_cz_wf_location = ba_cz_wf_location.drop_duplicates(subset=['ba_code','in.weather_file_city'])\n",
    "# rename the column\n",
    "ba_cz_wf_location = ba_cz_wf_location.rename(columns={'in.weather_file_city':'weather_file_city'})\n",
    "# export the crosswalk\n",
    "ba_cz_wf_location = ba_cz_wf_location.reset_index()[['ba_code','climate_zone','ba_name','weather_file_city']]\n",
    "ba_cz_wf_location.to_csv('../data/processed/ba_weather_file_crosswalk.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbfb11d76ba7fcd62de728492717c2999a3c11ae1c83ea329d6e038c7a5b35c0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('emissions': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
