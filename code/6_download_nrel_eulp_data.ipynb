{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from os import path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download EULP Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet_from_s3(url, download_path, file_name):\n",
    "    \"\"\"\n",
    "    Given a url to a parquet file on an amazon S3 bucket, downloads the file to a local folder\n",
    "    \"\"\"\n",
    "    file_object = requests.get(url)\n",
    "\n",
    "    with open(f'{download_path}/{file_name}', 'wb') as local_file:\n",
    "        local_file.write(file_object.content)\n",
    "\n",
    "com_meta_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/metadata/metadata.parquet'\n",
    "res_meta_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tm3_release_1/metadata/metadata.parquet'\n",
    "\n",
    "# download commercial metadata and convert to csv\n",
    "download_parquet_from_s3(com_meta_url, '../data/downloaded/eulp_usbs/metadata', 'commercial_metadata.parquet')\n",
    "download_parquet_from_s3(res_meta_url, '../data/downloaded/eulp_usbs/metadata', 'residential_metadata.parquet')\n",
    "\n",
    "res_metadata = pd.read_parquet('../data/downloaded/eulp_usbs/residential_metadata.parquet')\n",
    "com_metadata = pd.read_parquet('../data/downloaded/eulp_usbs/commercial_metadata.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Commercial data dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/data_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/com_data_dictionary.csv')\n",
    "\n",
    "# download enumeration dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/enumeration_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/com_enumeration_dictionary.csv')\n",
    "\n",
    "# download upgrade dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/upgrade_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/com_upgrade_dictionary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download Residential Data Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/data_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/res_data_dictionary.csv')\n",
    "\n",
    "# download enumeration dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/enumeration_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/res_enumeration_dictionary.csv')\n",
    "\n",
    "# download upgrade dictionary and save as csv\n",
    "dict_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/upgrade_dictionary.tsv'\n",
    "data_dictionary = pd.read_csv(dict_url, sep='\\t')\n",
    "data_dictionary.to_csv('../data/downloaded/eulp_usbs/metadata/res_upgrade_dictionary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take stratified random sample of commercial buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the commercial metadata file\n",
    "com_meta_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/metadata/metadata.parquet'\n",
    "com_metadata = pd.read_parquet(com_meta_url, columns=['bldg_id','in.upgrade_name','in.building_type','in.nhgis_tract_gisjoin','in.climate_zone_ashrae_2004','in.state_abbreviation'])\n",
    "\n",
    "# only keep buildings in Baseline upgrade\n",
    "com_metadata = com_metadata[com_metadata['in.upgrade_name'] == 'Baseline']\n",
    "com_metadata = com_metadata.drop(columns='in.upgrade_name')\n",
    "\n",
    "# rename columns\n",
    "com_metadata = com_metadata.rename(columns={'in.building_type':'building_type','in.nhgis_tract_gisjoin':'nhgis_tract_gisjoin','in.climate_zone_ashrae_2004':'climate_zone','in.state_abbreviation':'state'})\n",
    "\n",
    "# specify the category for each building type\n",
    "building_categories = {'FullServiceRestaurant': 'Restaurant',\n",
    "                        'Hospital': 'Hospital',\n",
    "                        'LargeHotel': 'Hotel',\n",
    "                        'LargeOffice': 'Office',\n",
    "                        'MediumOffice': 'Office',\n",
    "                        'OutPatient': 'Office',\n",
    "                        'Outpatient': 'Office',\n",
    "                        'PrimarySchool': 'School',\n",
    "                        'QuickServiceRestaurant': 'Restaurant',\n",
    "                        'RetailStandalone': 'Retail',\n",
    "                        'RetailStripmall': 'Retail',\n",
    "                        'SecondarySchool': 'School',\n",
    "                        'SmallHotel': 'Hotel',\n",
    "                        'SmallOffice': 'Office',\n",
    "                        'Warehouse': 'Warehouse'}\n",
    "\n",
    "com_metadata['building_category'] = com_metadata['building_type'].map(building_categories)\n",
    "\n",
    "com_metadata['building_sector'] = 'Commercial'\n",
    "\n",
    "com_metadata['scaling_factor'] = 1\n",
    "\n",
    "com_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the crosswalk between ba and tract\n",
    "ba_tract_crosswalk = pd.read_csv('../data/processed/ba_tract_crosswalk_2019.csv')\n",
    "\n",
    "ba_tract_crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a container for the commercial sample data\n",
    "commercial_sample = []\n",
    "\n",
    "# for each ba\n",
    "for ba in list(ba_tract_crosswalk['ba_code'].unique()):\n",
    "    # get a list of all tracts in the ba\n",
    "    ba_tract_list = list(ba_tract_crosswalk.loc[ba_tract_crosswalk['ba_code'] == ba, 'GISJOIN'].unique())\n",
    "    # find all buildings located in these tracts\n",
    "    buildings_in_ba = com_metadata.loc[com_metadata['nhgis_tract_gisjoin'].isin(ba_tract_list),:]\n",
    "    # sample 10% of buildings of each building type in each climate zone\n",
    "    frac_sample = buildings_in_ba.groupby(['climate_zone','building_type']).sample(frac=0.1, random_state=2022)\n",
    "    # take an n=1 sample from each building type in each climate zone\n",
    "    n_sample = buildings_in_ba.groupby(['climate_zone','building_type']).sample(n=1, random_state=2022)\n",
    "    # identify all of the unique CZ-buildingtype combinations that exist in each sample\n",
    "    n_sample['match_key'] = n_sample[['climate_zone','building_type']].agg('_'.join, axis=1)\n",
    "    n_unique_types = list(n_sample['match_key'].unique())\n",
    "    frac_sample['match_key'] = frac_sample[['climate_zone','building_type']].agg('_'.join, axis=1)\n",
    "    frac_unique_types = list(frac_sample['match_key'].unique())\n",
    "    # identify if there are any building types missing from the fractional sample\n",
    "    missing_sample = list(set(frac_unique_types).symmetric_difference(set(n_unique_types)))\n",
    "\n",
    "    # append any missing samples to the fractional sample from the n sample\n",
    "    if len(missing_sample) > 0:\n",
    "        frac_sample = pd.concat([frac_sample, n_sample[n_sample['match_key'].isin(missing_sample)]], axis=0)\n",
    "\n",
    "    # add a column for the ba code\n",
    "    frac_sample['ba_code'] = ba\n",
    "\n",
    "    commercial_sample.append(frac_sample)\n",
    "\n",
    "commercial_sample = pd.concat(commercial_sample, axis=0)\n",
    "\n",
    "commercial_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take stratified random sample of residential buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the residential metadata file\n",
    "res_meta_url = 'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/metadata/metadata.parquet'\n",
    "res_metadata = pd.read_parquet(res_meta_url, columns=['bldg_id','in.weather_file_city','in.ashrae_iecc_climate_zone_2004','in.geometry_building_type_acs','in.geometry_building_number_units_mf','in.geometry_building_number_units_sfa','in.units_represented','in.state'])\n",
    "\n",
    "res_building_categories = {'Mobile Home':'MobileHome',\n",
    "                           'Single-Family Attached':'SingleFamily',\n",
    "                           'Single-Family Detached':'SingleFamily',\n",
    "                           '2 Unit':'SmallMultifamily',\n",
    "                           '3 or 4 Unit':'SmallMultifamily',\n",
    "                           '5 to 9 Unit':'MediumMultifamily',\n",
    "                           '10 to 19 Unit':'MediumMultifamily',\n",
    "                           '20 to 49 Unit':'MediumMultifamily',\n",
    "                           '50 or more Unit':'LargeMultifamily'}\n",
    "\n",
    "res_building_names = {'Mobile Home':'MobileHome',\n",
    "                           'Single-Family Attached':'SFAttached',\n",
    "                           'Single-Family Detached':'SFDetached',\n",
    "                           '2 Unit':'MF2unit',\n",
    "                           '3 or 4 Unit':'MF3-4unit',\n",
    "                           '5 to 9 Unit':'MF5-9unit',\n",
    "                           '10 to 19 Unit':'MF10-19unit',\n",
    "                           '20 to 49 Unit':'MF20-49unit',\n",
    "                           '50 or more Unit':'MF50+unit'}\n",
    "\n",
    "# rename the building categories in the building type acs column\n",
    "res_metadata['building_category'] = res_metadata['in.geometry_building_type_acs'].map(res_building_categories)\n",
    "\n",
    "# calculate a scaling factor based on the number of units represented by the data\n",
    "res_metadata['in.geometry_building_number_units_mf'] = res_metadata['in.geometry_building_number_units_mf'].replace({'None':0}).astype(int)\n",
    "res_metadata['in.geometry_building_number_units_sfa'] = res_metadata['in.geometry_building_number_units_sfa'].replace({'None':0}).astype(int)\n",
    "res_metadata['scaling_factor'] = ((res_metadata['in.geometry_building_number_units_mf'] + res_metadata['in.geometry_building_number_units_sfa']) / res_metadata['in.units_represented']).replace({0:1})\n",
    "res_metadata = res_metadata.drop(columns=['in.geometry_building_number_units_mf','in.geometry_building_number_units_sfa','in.units_represented'])\n",
    "\n",
    "res_metadata = res_metadata.rename(columns={'in.weather_file_city':'weather_file_city','in.geometry_building_type_acs':'building_type','in.state':'state','in.ashrae_iecc_climate_zone_2004':'climate_zone'})\n",
    "\n",
    "# rename the building categories in the building type acs column\n",
    "res_metadata['building_type'] = res_metadata['building_type'].replace(res_building_names)\n",
    "\n",
    "res_metadata['building_sector'] = 'Residential'\n",
    "\n",
    "res_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the crosswalk between ba and tract\n",
    "ba_wf_crosswalk = pd.read_csv('../data/processed/ba_weather_file_crosswalk.csv')\n",
    "\n",
    "ba_wf_crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a container for the commercial sample data\n",
    "residential_sample = []\n",
    "\n",
    "# for each ba\n",
    "for ba in list(ba_wf_crosswalk['ba_code'].unique()):\n",
    "    # get a list of all weather files in the ba\n",
    "    ba_wf_list = list(ba_wf_crosswalk.loc[ba_wf_crosswalk['ba_code'] == ba, 'weather_file_city'].unique())\n",
    "    # find all buildings located in these tracts\n",
    "    buildings_in_ba = res_metadata.loc[res_metadata['weather_file_city'].isin(ba_wf_list),:]\n",
    "    # sample 10% of buildings of each building type in each climate zone\n",
    "    frac_sample = buildings_in_ba.groupby(['climate_zone','building_type']).sample(frac=0.1, random_state=2022)\n",
    "    # take an n=1 sample from each building type in each climate zone\n",
    "    n_sample = buildings_in_ba.groupby(['climate_zone','building_type']).sample(n=1, random_state=2022)\n",
    "    # identify all of the unique CZ-buildingtype combinations that exist in each sample\n",
    "    n_sample['match_key'] = n_sample[['climate_zone','building_type']].agg('_'.join, axis=1)\n",
    "    n_unique_types = list(n_sample['match_key'].unique())\n",
    "    frac_sample['match_key'] = frac_sample[['climate_zone','building_type']].agg('_'.join, axis=1)\n",
    "    frac_unique_types = list(frac_sample['match_key'].unique())\n",
    "    # identify if there are any building types missing from the fractional sample\n",
    "    missing_sample = list(set(frac_unique_types).symmetric_difference(set(n_unique_types)))\n",
    "\n",
    "    # append any missing samples to the fractional sample from the n sample\n",
    "    if len(missing_sample) > 0:\n",
    "        frac_sample = pd.concat([frac_sample, n_sample[n_sample['match_key'].isin(missing_sample)]], axis=0)\n",
    "\n",
    "    # add a column for the ba code\n",
    "    frac_sample['ba_code'] = ba\n",
    "\n",
    "    residential_sample.append(frac_sample)\n",
    "\n",
    "residential_sample = pd.concat(residential_sample, axis=0)\n",
    "\n",
    "residential_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ba in ba_list:\n",
    "    # check if the data has already been downloaded\n",
    "    if os.path.exists(f'../data/processed/nrel_demand/{ba}.csv.zip'):\n",
    "        print(f'BA {ba} already downloaded.')\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # create a list to hold the data\n",
    "        commercial_data = []\n",
    "        residential_data = []\n",
    "        already_downloaded = []\n",
    "\n",
    "        com_buildings = commercial_sample.loc[commercial_sample['ba_code'] == ba,:]\n",
    "        num_com = len(com_buildings)\n",
    "        \n",
    "        print(f'Downloading {num_com} {ba} commercial buildings')\n",
    "        for bldg_id, row in com_buildings.iterrows():\n",
    "            if bldg_id in already_downloaded:\n",
    "                pass\n",
    "            else:\n",
    "                print(f'  Downloading bldg_id {bldg_id}',end='\\r')\n",
    "                state = row['state']\n",
    "                cz = row['climate_zone']\n",
    "                building_category = row['building_category']\n",
    "                building_type = row['building_type']\n",
    "\n",
    "                # construct the url for the individual building timeseries file\n",
    "                url_to_download = f'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/comstock_tmy3_release_1/timeseries_individual_buildings/by_state/upgrade=0/state={state}/{bldg_id}-0.parquet'\n",
    "\n",
    "                # read the building data into a dataframe, keeping only the total electricity timeseries data\n",
    "                try:\n",
    "                    bldg_data = pd.read_parquet(url_to_download, columns=['timestamp','out.electricity.total.energy_consumption'])\n",
    "                    # resample the data from 15 min interval to 1 hour interval, and round to 2 decimal points\n",
    "                    bldg_data = bldg_data.set_index(pd.to_datetime(bldg_data['timestamp']), drop=True).resample('H', label='left', closed='right').sum().round(2)\n",
    "\n",
    "                    # rename the column to describe the building\n",
    "                    bldg_data = bldg_data.rename(columns={'out.electricity.total.energy_consumption':f'{cz}_{building_category}_{building_type}_{bldg_id}'})\n",
    "\n",
    "                    commercial_data.append(bldg_data)\n",
    "                    already_downloaded.append(bldg_id)\n",
    "                except:\n",
    "                    try:\n",
    "                        print(f'ERROR DOWNLOADING {bldg_id}. Retrying...')\n",
    "                        bldg_data = pd.read_parquet(url_to_download, columns=['timestamp','out.electricity.total.energy_consumption'])\n",
    "                        # resample the data from 15 min interval to 1 hour interval, and round to 2 decimal points\n",
    "                        bldg_data = bldg_data.set_index(pd.to_datetime(bldg_data['timestamp']), drop=True).resample('H', label='left', closed='right').sum().round(2)\n",
    "\n",
    "                        # rename the column to describe the building\n",
    "                        bldg_data = bldg_data.rename(columns={'out.electricity.total.energy_consumption':f'{cz}_{building_category}_{building_type}_{bldg_id}'})\n",
    "\n",
    "                        commercial_data.append(bldg_data)\n",
    "                        already_downloaded.append(bldg_id)\n",
    "                    except:\n",
    "                        print(f'SKIPPING {bldg_id}')\n",
    "\n",
    "        \n",
    "\n",
    "        res_buildings = residential_sample.loc[residential_sample['ba_code'] == ba,:]\n",
    "        num_res = len(res_buildings)\n",
    "\n",
    "        print(f'Downloading {num_res} {ba} residential buildings')\n",
    "        for bldg_id, row in res_buildings.iterrows():\n",
    "            if bldg_id in already_downloaded:\n",
    "                pass\n",
    "            else:\n",
    "                print(f'  Downloading bldg_id {bldg_id}',end='\\r')\n",
    "                state = row['state']\n",
    "                cz = row['climate_zone']\n",
    "                building_category = row['building_category']\n",
    "                building_type = row['building_type']\n",
    "\n",
    "                # construct the url for the individual building timeseries file\n",
    "                url_to_download = f'https://oedi-data-lake.s3.amazonaws.com/nrel-pds-building-stock/end-use-load-profiles-for-us-building-stock/2021/resstock_tmy3_release_1/timeseries_individual_buildings/by_state/upgrade=0/state={state}/{bldg_id}-0.parquet'\n",
    "\n",
    "                # read the building data into a dataframe, keeping only the total electricity timeseries data\n",
    "                try:\n",
    "                    bldg_data = pd.read_parquet(url_to_download, columns=['timestamp','out.electricity.total.energy_consumption'])\n",
    "                    # resample the data from 15 min interval to 1 hour interval, and round to 2 decimal points\n",
    "                    bldg_data = bldg_data.set_index(pd.to_datetime(bldg_data['timestamp']), drop=True).resample('H', label='left', closed='right').sum().round(2)\n",
    "\n",
    "                    # scale the data\n",
    "                    bldg_data['out.electricity.total.energy_consumption'] = bldg_data['out.electricity.total.energy_consumption'] * row['scaling_factor']\n",
    "\n",
    "                    # rename the column to describe the building\n",
    "                    bldg_data = bldg_data.rename(columns={'out.electricity.total.energy_consumption':f'{cz}_{building_category}_{building_type}_{bldg_id}'})\n",
    "\n",
    "                    residential_data.append(bldg_data)\n",
    "                    already_downloaded.append(bldg_id)\n",
    "                except:\n",
    "                    try:\n",
    "                        print(f'ERROR DOWNLOADING {bldg_id}. Retrying...')\n",
    "                        bldg_data = pd.read_parquet(url_to_download, columns=['timestamp','out.electricity.total.energy_consumption'])\n",
    "                        # resample the data from 15 min interval to 1 hour interval, and round to 2 decimal points\n",
    "                        bldg_data = bldg_data.set_index(pd.to_datetime(bldg_data['timestamp']), drop=True).resample('H', label='left', closed='right').sum().round(2)\n",
    "\n",
    "                        # scale the data\n",
    "                        bldg_data['out.electricity.total.energy_consumption'] = bldg_data['out.electricity.total.energy_consumption'] * row['scaling_factor']\n",
    "\n",
    "                        # rename the column to describe the building\n",
    "                        bldg_data = bldg_data.rename(columns={'out.electricity.total.energy_consumption':f'{cz}_{building_category}_{building_type}_{bldg_id}'})\n",
    "\n",
    "                        residential_data.append(bldg_data)\n",
    "                        already_downloaded.append(bldg_id)\n",
    "                    except:\n",
    "                        print(f'SKIPPING {bldg_id}')\n",
    "                \n",
    "\n",
    "        print('  Constructing dataframe')\n",
    "\n",
    "        commercial_data = pd.concat(commercial_data, axis='columns')\n",
    "        residential_data = pd.concat(residential_data, axis='columns')\n",
    "\n",
    "        demand_data = pd.concat([commercial_data, residential_data], axis='columns')\n",
    "            \n",
    "        # rename the timestamp column to datetime_local\n",
    "        demand_data = demand_data.reset_index(drop=True)\n",
    "        \n",
    "        # save the data as a zipped csv\n",
    "        demand_data.to_csv(f'../data/processed/nrel_demand/{ba}.csv.zip', compression='zip', index=False)\n",
    "\n",
    "        # reset the dataframe\n",
    "        demand_data = pd.DataFrame()\n",
    "\n",
    "        print(f'  {round(time.time() - start_time, 0)} s / {round(time.time() - start_time, 2) / (num_com + num_res)} s/bldg')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbfb11d76ba7fcd62de728492717c2999a3c11ae1c83ea329d6e038c7a5b35c0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('emissions': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
